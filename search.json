[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/day1/index.html",
    "href": "posts/day1/index.html",
    "title": "Day 1",
    "section": "",
    "text": "This is my first blog post! Here are my reflections and notes from day 1.\n\n\n\nWe talked about good practice in storing and processing data\nHow to set up a Quarto blog\nThe importance of YAML frontmatter\nHow to write content in Markdown\n\n\n\n\n\nWe discussed the data lifecycle and what different stages the data goes through. from planning for what data to be collected to eventually being reused. The data lifecycle can be long and involve multiple people and thats why its important to work according to FAIR-principles.\nFAIR - Findable Accesible Interoperable and Reusable. Working according to this concept ensure that data collected for one question can be used for many more.\n\nThis relies on good data management practices where you work by research documentation, data organisation, information security, ethics and legislation.\n\nData is often not complete after first collection, first processing or first analysis. things are being added or moved between project altered as the work goes along. which makes it important to store data in a good system in a separate folder of raw-data which you dont touch.\n\nsome tips for good practice with documenting your data:\n\nDocument your methods and workflows.\nDocument where and when you downloaded data.\nDocument the versions of the software that you ran.\n\n\nOther things that we will practice later in the course is:\n\nEmploying version control via GIT\nShareable environments to make your scripts reproducible.\ncontainers to run scripts on other operating systems\nworkflow managers, that keep track of all of the above and also parse your data through the different scripts.\n\n\n\n\n\n\nWe also learned to set up this blog!\nQuarto allows you to write text and code into the same document and render into a few different formats such as webpages, blogs (!), docx and pdf.\n\n\n\n\n\nhere is a python example:\n\n\n# A simple Python example\ngreeting = \"Hello, world!\"\nprint(greeting)\n\nHello, world!\n\n# Some basic math\nnumbers = [1, 2, 3, 4, 5]\ntotal = sum(numbers)\nprint(f\"The sum is: {total}\")\n\nThe sum is: 15\n\n\n\nand this is a R example!\n\n\ndata(mtcars)\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThat‚Äôs all for today!"
  },
  {
    "objectID": "posts/day1/index.html#what-i-learned-today",
    "href": "posts/day1/index.html#what-i-learned-today",
    "title": "Day 1",
    "section": "",
    "text": "We talked about good practice in storing and processing data\nHow to set up a Quarto blog\nThe importance of YAML frontmatter\nHow to write content in Markdown"
  },
  {
    "objectID": "posts/day1/index.html#data-management-and-reproducible-research",
    "href": "posts/day1/index.html#data-management-and-reproducible-research",
    "title": "Day 1",
    "section": "",
    "text": "We discussed the data lifecycle and what different stages the data goes through. from planning for what data to be collected to eventually being reused. The data lifecycle can be long and involve multiple people and thats why its important to work according to FAIR-principles.\nFAIR - Findable Accesible Interoperable and Reusable. Working according to this concept ensure that data collected for one question can be used for many more.\n\nThis relies on good data management practices where you work by research documentation, data organisation, information security, ethics and legislation.\n\nData is often not complete after first collection, first processing or first analysis. things are being added or moved between project altered as the work goes along. which makes it important to store data in a good system in a separate folder of raw-data which you dont touch.\n\nsome tips for good practice with documenting your data:\n\nDocument your methods and workflows.\nDocument where and when you downloaded data.\nDocument the versions of the software that you ran.\n\n\nOther things that we will practice later in the course is:\n\nEmploying version control via GIT\nShareable environments to make your scripts reproducible.\ncontainers to run scripts on other operating systems\nworkflow managers, that keep track of all of the above and also parse your data through the different scripts."
  },
  {
    "objectID": "posts/day1/index.html#quarto-blog",
    "href": "posts/day1/index.html#quarto-blog",
    "title": "Day 1",
    "section": "",
    "text": "We also learned to set up this blog!\nQuarto allows you to write text and code into the same document and render into a few different formats such as webpages, blogs (!), docx and pdf."
  },
  {
    "objectID": "posts/day1/index.html#here-are-some-samples-of-code",
    "href": "posts/day1/index.html#here-are-some-samples-of-code",
    "title": "Day 1",
    "section": "",
    "text": "here is a python example:\n\n\n# A simple Python example\ngreeting = \"Hello, world!\"\nprint(greeting)\n\nHello, world!\n\n# Some basic math\nnumbers = [1, 2, 3, 4, 5]\ntotal = sum(numbers)\nprint(f\"The sum is: {total}\")\n\nThe sum is: 15\n\n\n\nand this is a R example!\n\n\ndata(mtcars)\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThat‚Äôs all for today!"
  },
  {
    "objectID": "posts/day2/index.html",
    "href": "posts/day2/index.html",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Welcome back to my learning journey! Here are my reflections and key takeaways from day 2.\n\n\nToday we covered four main topics: - Introduction to environments - Quality control with Pixi and Slurm - Introduction to containers\n- Quality control with containers\n\n\n\nSince I work with variant visualisations, large datasets, and a mix of custom scripts and publication-ready figures it‚Äôs crucial that my toolchain is consistent and reproducible across systems and collaborators. Using Pixi (or another environment manager) keeps everything isolated and version-controlled, so I don‚Äôt end up with ‚Äúit works on my computer‚Äù issues. Having a dedicated environment for each project makes it easier to share my setup ‚Äî others can just clone the repository and get the exact same tools and versions. It also prevents older pipelines or scripts from breaking when dependencies update. When I prepare figures or analyses for papers, Pixi can help me keep track of software versions, channels, and dependencies ‚Äî which makes the ‚ÄúMethods‚Äù section reproducible. Pixi supports multiple platforms and I can test everything locally and then run it on the cluster with minimal risk of environment mismatch.\n\n\n\nQuality control isn‚Äôt the most exciting part of bioinformatics, but it‚Äôs one of the most important. If the sequencing data is bad, everything that comes after will be unreliable. Running FastQC and MultiQC early on saves time later, even if the results themselves are pretty basic. It also makes me think more about how I organize things. Having a clear directory structure and running everything inside a controlled environment like Pixi helps keep track of what was actually done. It‚Äôs easy to lose that overview once the project grows. Working with SLURM and screen is a bit clunky at first, but it‚Äôs useful to understand how jobs run on the cluster. Being able to rerun things reproducibly ‚Äî same data, same tools, same parameters ‚Äî is really the main point.\n\n\n\nReproducibility is a constant problem in bioinformatics, and containers make it more manageable. They keep analyses stable even when systems or dependencies change. It‚Äôs also a practical way to share workflows without needing to explain every installation step. Building containers from scratch feels a bit advanced, but understanding how they work makes the overall workflow more reliable.\n\n*That‚Äôs all for today"
  },
  {
    "objectID": "posts/day2/index.html#todays-focus",
    "href": "posts/day2/index.html#todays-focus",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Today we covered four main topics: - Introduction to environments - Quality control with Pixi and Slurm - Introduction to containers\n- Quality control with containers"
  },
  {
    "objectID": "posts/day2/index.html#introduction-to-environments",
    "href": "posts/day2/index.html#introduction-to-environments",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Since I work with variant visualisations, large datasets, and a mix of custom scripts and publication-ready figures it‚Äôs crucial that my toolchain is consistent and reproducible across systems and collaborators. Using Pixi (or another environment manager) keeps everything isolated and version-controlled, so I don‚Äôt end up with ‚Äúit works on my computer‚Äù issues. Having a dedicated environment for each project makes it easier to share my setup ‚Äî others can just clone the repository and get the exact same tools and versions. It also prevents older pipelines or scripts from breaking when dependencies update. When I prepare figures or analyses for papers, Pixi can help me keep track of software versions, channels, and dependencies ‚Äî which makes the ‚ÄúMethods‚Äù section reproducible. Pixi supports multiple platforms and I can test everything locally and then run it on the cluster with minimal risk of environment mismatch."
  },
  {
    "objectID": "posts/day2/index.html#quality-control-with-pixi-and-slurm",
    "href": "posts/day2/index.html#quality-control-with-pixi-and-slurm",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Quality control isn‚Äôt the most exciting part of bioinformatics, but it‚Äôs one of the most important. If the sequencing data is bad, everything that comes after will be unreliable. Running FastQC and MultiQC early on saves time later, even if the results themselves are pretty basic. It also makes me think more about how I organize things. Having a clear directory structure and running everything inside a controlled environment like Pixi helps keep track of what was actually done. It‚Äôs easy to lose that overview once the project grows. Working with SLURM and screen is a bit clunky at first, but it‚Äôs useful to understand how jobs run on the cluster. Being able to rerun things reproducibly ‚Äî same data, same tools, same parameters ‚Äî is really the main point."
  },
  {
    "objectID": "posts/day2/index.html#introduction-to-containers",
    "href": "posts/day2/index.html#introduction-to-containers",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Reproducibility is a constant problem in bioinformatics, and containers make it more manageable. They keep analyses stable even when systems or dependencies change. It‚Äôs also a practical way to share workflows without needing to explain every installation step. Building containers from scratch feels a bit advanced, but understanding how they work makes the overall workflow more reliable.\n\n*That‚Äôs all for today"
  },
  {
    "objectID": "posts/day4/index.html",
    "href": "posts/day4/index.html",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "Day 4 brought us deeper into the nf-core ecosystem! Today we moved from basic Nextflow to advanced pipeline usage with real biological data.\n\n\nToday we worked with: - nf-core pipeline introduction and evaluation - Testing pipelines with built-in test data\n- Running the nf-core/rnaseq pipeline on real data - Understanding pipeline configuration and job submission\n\n\n\n\n\nThe nf-core community provides: - Standardized pipelines for many bioinformatics workflows - Extensive documentation following consistent guidelines - Automatic input validation through the nf-core launcher - Consistent structure across all pipelines - Open source development by volunteer community\n\n\n\nWe explored the nf-core homepage and learned to evaluate pipelines by checking: - Usage documentation - understanding input requirements - Parameters - available customization options\n- Output - expected results and file formats - Pipeline suitability for our specific data types\n\n\n\n\n\n\nFirst, we created a dedicated pixi environment for nf-core work:\npixi init nextflow_test -c conda-forge -c bioconda\ncd nextflow_test\npixi add nextflow nf-core\n\n\n\nWe verified our installation worked correctly:\npixi run nextflow -version\npixi run nf-core --help\npixi run nextflow run hello\nExpected output:\nN E X T F L O W\nversion 25.04.7 build 5955\ncreated 08-09-2025 13:29 UTC (15:29 CEST)\ncite doi:10.1038/nbt.3820\nhttp://nextflow.io\n\n\n\nWe downloaded the HPC2N configuration file for server-specific settings:\nwget https://raw.githubusercontent.com/hpc2n/intro-course/master/exercises/NEXTFLOW/INTERACTIVE/hpc2n.config\nKey configuration parameters: - max_memory = 128.GB - max_cpus = 28 - max_time = 168.h - executor = 'slurm'\n\n\n\nWe tested the Sarek variant calling pipeline with built-in test data:\npixi run nextflow run nf-core/sarek -profile test --outdir sarek_test -c hpc2n.config\nCommand breakdown: - pixi run - Use our pixi environment - nextflow run - Execute with Nextflow - nf-core/sarek - Pipeline name/location - -profile test - Use built-in test data - --outdir sarek_test - Output directory - -c hpc2n.config - Server configuration file\nThe test run completed in ~3.5 minutes, demonstrating successful setup.\n\n\n\n\n\n\nFor real analysis, we worked with RNAseq data located at:\nmedbioinfo2025/common_data/RNAseq\nWe created symbolic links to organize our data:\nmkdir data\ncd data\nln -s ../../common_data/RNAseq/*.fastq.gz .\ncd ..\nProject structure:\n.\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223504_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_1.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223504_2.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_2.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223517_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223517_1.fastq.gz\n‚îÇ   ‚îî‚îÄ‚îÄ ... (additional samples)\n‚îú‚îÄ‚îÄ pixi.lock\n‚îî‚îÄ‚îÄ pixi.toml\n\n\n\nWe used the nf-core launcher to configure the RNAseq pipeline:\n\nWorking and results directories - Set absolute paths\nInput CSV file - Created samplesheet according to pipeline requirements\n\nReference genome - Used updated human genome references\nResume option - Enabled for fault tolerance\n\n\n\n\nTwo execution methods were available:\n\n\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\n\n\n\n#!/bin/bash -l\n#SBATCH -A our_proj_allocation\n#SBATCH -n 5\n#SBATCH -t 24:00:00\n\n/your_home_directory/.pixi/bin/pixi run nextflow run nf-core/rnaseq -r 3.19.0 -params-file /your_path/nf-params.json -c server.config\n\n\n\n\n\n\n\n\nPipeline evaluation - How to assess nf-core pipelines for your data\nEnvironment management - Using pixi for reproducible setups\nConfiguration management - Server-specific settings and parameter files\nJob submission - Both interactive and batch execution methods\nData organization - Proper file linking and project structure\n\n\n\n\n\nCommunity-driven development - Volunteer-maintained pipelines\nStandardization benefits - Consistent documentation and structure\nQuality control importance - Understanding pipeline suitability for data\nReproducibility features - Parameter files and configuration tracking\n\n\n\n\n\n\n\nProblem: Understanding which pipeline is suitable for my specific data type\nSolution: Systematic evaluation using nf-core documentation sections (Usage, Parameters, Output)\n\n\n\nProblem: Configuration file setup for server-specific requirements\nSolution: Download and customize HPC2N configuration template with appropriate resource limits\n\n\n\nProblem: Managing complex parameter files for pipeline runs\nSolution: Use nf-core launcher to generate JSON parameter files automatically\n\n\n\n\n\n‚Äúnf-core transforms bioinformatics from custom scripting to standardized, reproducible workflows‚Äù\n\n\nBuilding on Day 3: We moved from basic Nextflow concepts to production-ready pipelines\nReal-world application: Working with actual RNAseq data instead of toy examples\n\nCommunity aspect: Understanding the collaborative nature of modern bioinformatics\nQuality focus: Emphasis on validation and understanding rather than black-box usage\n\n\n\n\n\nDay 1: Data management and FAIR principles ‚Üí Foundation for reproducible research\nDay 2: Environments and quality control ‚Üí Tools for reliable analysis\nDay 3: Nextflow and workflow management ‚Üí Core workflow technology\nDay 4: nf-core pipelines and RNAseq ‚Üí Production bioinformatics applications\n\n\n\n\n\nHow do I evaluate if a pipeline is actively maintained and suitable for my research?\nWhat are best practices for customizing nf-core pipelines for specific research needs?\nHow can I contribute back to the nf-core community with improvements or new modules?\n\n\n\n\nGoals for Day 5: - [ ] Analyze RNAseq pipeline output in detail - [ ] Explore other nf-core pipelines relevant to my research - [ ] Practice parameter optimization for real datasets\n\n\n\n\nnf-core homepage\nnf-core/rnaseq pipeline\nnf-core/sarek pipeline\nHPC2N configuration files\nCourse materials\n\n\nDay 4 complete! From simple workflows to production pipelines - the power of standardized bioinformatics is incredible. üß¨üöÄ"
  },
  {
    "objectID": "posts/day4/index.html#todays-focus",
    "href": "posts/day4/index.html#todays-focus",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "Today we worked with: - nf-core pipeline introduction and evaluation - Testing pipelines with built-in test data\n- Running the nf-core/rnaseq pipeline on real data - Understanding pipeline configuration and job submission"
  },
  {
    "objectID": "posts/day4/index.html#morning-session---nf-core-introduction",
    "href": "posts/day4/index.html#morning-session---nf-core-introduction",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "The nf-core community provides: - Standardized pipelines for many bioinformatics workflows - Extensive documentation following consistent guidelines - Automatic input validation through the nf-core launcher - Consistent structure across all pipelines - Open source development by volunteer community\n\n\n\nWe explored the nf-core homepage and learned to evaluate pipelines by checking: - Usage documentation - understanding input requirements - Parameters - available customization options\n- Output - expected results and file formats - Pipeline suitability for our specific data types"
  },
  {
    "objectID": "posts/day4/index.html#afternoon-session---hands-on-pipeline-testing",
    "href": "posts/day4/index.html#afternoon-session---hands-on-pipeline-testing",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "First, we created a dedicated pixi environment for nf-core work:\npixi init nextflow_test -c conda-forge -c bioconda\ncd nextflow_test\npixi add nextflow nf-core\n\n\n\nWe verified our installation worked correctly:\npixi run nextflow -version\npixi run nf-core --help\npixi run nextflow run hello\nExpected output:\nN E X T F L O W\nversion 25.04.7 build 5955\ncreated 08-09-2025 13:29 UTC (15:29 CEST)\ncite doi:10.1038/nbt.3820\nhttp://nextflow.io\n\n\n\nWe downloaded the HPC2N configuration file for server-specific settings:\nwget https://raw.githubusercontent.com/hpc2n/intro-course/master/exercises/NEXTFLOW/INTERACTIVE/hpc2n.config\nKey configuration parameters: - max_memory = 128.GB - max_cpus = 28 - max_time = 168.h - executor = 'slurm'\n\n\n\nWe tested the Sarek variant calling pipeline with built-in test data:\npixi run nextflow run nf-core/sarek -profile test --outdir sarek_test -c hpc2n.config\nCommand breakdown: - pixi run - Use our pixi environment - nextflow run - Execute with Nextflow - nf-core/sarek - Pipeline name/location - -profile test - Use built-in test data - --outdir sarek_test - Output directory - -c hpc2n.config - Server configuration file\nThe test run completed in ~3.5 minutes, demonstrating successful setup."
  },
  {
    "objectID": "posts/day4/index.html#advanced-topic---rnaseq-pipeline",
    "href": "posts/day4/index.html#advanced-topic---rnaseq-pipeline",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "For real analysis, we worked with RNAseq data located at:\nmedbioinfo2025/common_data/RNAseq\nWe created symbolic links to organize our data:\nmkdir data\ncd data\nln -s ../../common_data/RNAseq/*.fastq.gz .\ncd ..\nProject structure:\n.\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223504_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_1.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223504_2.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_2.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223517_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223517_1.fastq.gz\n‚îÇ   ‚îî‚îÄ‚îÄ ... (additional samples)\n‚îú‚îÄ‚îÄ pixi.lock\n‚îî‚îÄ‚îÄ pixi.toml\n\n\n\nWe used the nf-core launcher to configure the RNAseq pipeline:\n\nWorking and results directories - Set absolute paths\nInput CSV file - Created samplesheet according to pipeline requirements\n\nReference genome - Used updated human genome references\nResume option - Enabled for fault tolerance\n\n\n\n\nTwo execution methods were available:\n\n\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\n\n\n\n#!/bin/bash -l\n#SBATCH -A our_proj_allocation\n#SBATCH -n 5\n#SBATCH -t 24:00:00\n\n/your_home_directory/.pixi/bin/pixi run nextflow run nf-core/rnaseq -r 3.19.0 -params-file /your_path/nf-params.json -c server.config"
  },
  {
    "objectID": "posts/day4/index.html#key-learnings",
    "href": "posts/day4/index.html#key-learnings",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "Pipeline evaluation - How to assess nf-core pipelines for your data\nEnvironment management - Using pixi for reproducible setups\nConfiguration management - Server-specific settings and parameter files\nJob submission - Both interactive and batch execution methods\nData organization - Proper file linking and project structure\n\n\n\n\n\nCommunity-driven development - Volunteer-maintained pipelines\nStandardization benefits - Consistent documentation and structure\nQuality control importance - Understanding pipeline suitability for data\nReproducibility features - Parameter files and configuration tracking"
  },
  {
    "objectID": "posts/day4/index.html#challenges-and-solutions",
    "href": "posts/day4/index.html#challenges-and-solutions",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "Problem: Understanding which pipeline is suitable for my specific data type\nSolution: Systematic evaluation using nf-core documentation sections (Usage, Parameters, Output)\n\n\n\nProblem: Configuration file setup for server-specific requirements\nSolution: Download and customize HPC2N configuration template with appropriate resource limits\n\n\n\nProblem: Managing complex parameter files for pipeline runs\nSolution: Use nf-core launcher to generate JSON parameter files automatically"
  },
  {
    "objectID": "posts/day4/index.html#insights-and-connections",
    "href": "posts/day4/index.html#insights-and-connections",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "‚Äúnf-core transforms bioinformatics from custom scripting to standardized, reproducible workflows‚Äù\n\n\nBuilding on Day 3: We moved from basic Nextflow concepts to production-ready pipelines\nReal-world application: Working with actual RNAseq data instead of toy examples\n\nCommunity aspect: Understanding the collaborative nature of modern bioinformatics\nQuality focus: Emphasis on validation and understanding rather than black-box usage"
  },
  {
    "objectID": "posts/day4/index.html#comparison-with-previous-days",
    "href": "posts/day4/index.html#comparison-with-previous-days",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "Day 1: Data management and FAIR principles ‚Üí Foundation for reproducible research\nDay 2: Environments and quality control ‚Üí Tools for reliable analysis\nDay 3: Nextflow and workflow management ‚Üí Core workflow technology\nDay 4: nf-core pipelines and RNAseq ‚Üí Production bioinformatics applications"
  },
  {
    "objectID": "posts/day4/index.html#questions-for-further-exploration",
    "href": "posts/day4/index.html#questions-for-further-exploration",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "How do I evaluate if a pipeline is actively maintained and suitable for my research?\nWhat are best practices for customizing nf-core pipelines for specific research needs?\nHow can I contribute back to the nf-core community with improvements or new modules?"
  },
  {
    "objectID": "posts/day4/index.html#looking-ahead",
    "href": "posts/day4/index.html#looking-ahead",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "Goals for Day 5: - [ ] Analyze RNAseq pipeline output in detail - [ ] Explore other nf-core pipelines relevant to my research - [ ] Practice parameter optimization for real datasets"
  },
  {
    "objectID": "posts/day4/index.html#resources-and-references",
    "href": "posts/day4/index.html#resources-and-references",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "nf-core homepage\nnf-core/rnaseq pipeline\nnf-core/sarek pipeline\nHPC2N configuration files\nCourse materials\n\n\nDay 4 complete! From simple workflows to production pipelines - the power of standardized bioinformatics is incredible. üß¨üöÄ"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/day3/index.html",
    "href": "posts/day3/index.html",
    "title": "Day 3 of MedBioInfo",
    "section": "",
    "text": "Welcome to Day 3! Here are my reflections and key takeaways from today‚Äôs sessions.\n\n\nToday we worked with Nextflow and workflow management.\n\n\n\nNextflow Overview\n\n\nFigure 1: Nextflow workflow management system overview. This diagram illustrates the core concepts of Nextflow including processes, channels, and how data flows through a computational pipeline. Nextflow enables scalable and portable bioinformatics workflows.\n\n\nToday we ran our first Nextflow script! Here‚Äôs the complete hello.nf script:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nWhen we run this script:\nnextflow run hello.nf\nWe get this output:\nN E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ‚úî\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ‚úî\nHELLO \nWORLD!\nThis workflow demonstrates several key Nextflow concepts: - Channels: greeting_ch carries data between processes - SPLITLETTERS process: Uses split command to break text into 6-byte chunks - CONVERTTOUPPER process: Converts text to uppercase using tr command - Workflow: Connects processes with .flatten() to handle multiple outputs - Output: Uses .view{ it } to display results\n\n\n\nOne of the features of Nextflow is the ability to override parameters from the command line. We can change the greeting message without modifying the script:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\n\n\n\nWe also worked with building our own nextflow pipelines for quality control of rna-seq data. The training included running FastQC and MultiQC on sample data:\n\n\n\nChannel Process FastQC\n\n\nFigure 2: Nextflow channel and process architecture for FastQC quality control. This diagram shows how input data flows through channels into FastQC processes. Each process can run independently while channels manage data flow between steps.\n\n\n\nFastQC Report - Sample 1 - Individual quality control report for gut sample 1\nFastQC Report - Sample 2 - Individual quality control report for gut sample 2\n\nMultiQC Report - Comprehensive report combining all QC metrics\n\nThese reports demonstrate: - FastQC analysis: Per-base quality scores, sequence composition, adapter contamination - MultiQC aggregation: Combined visualization of multiple samples\n\nEnd of Day 3!"
  },
  {
    "objectID": "posts/day3/index.html#todays-focus",
    "href": "posts/day3/index.html#todays-focus",
    "title": "Day 3 of MedBioInfo",
    "section": "",
    "text": "Today we worked with Nextflow and workflow management.\n\n\n\nNextflow Overview\n\n\nFigure 1: Nextflow workflow management system overview. This diagram illustrates the core concepts of Nextflow including processes, channels, and how data flows through a computational pipeline. Nextflow enables scalable and portable bioinformatics workflows.\n\n\nToday we ran our first Nextflow script! Here‚Äôs the complete hello.nf script:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nWhen we run this script:\nnextflow run hello.nf\nWe get this output:\nN E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ‚úî\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ‚úî\nHELLO \nWORLD!\nThis workflow demonstrates several key Nextflow concepts: - Channels: greeting_ch carries data between processes - SPLITLETTERS process: Uses split command to break text into 6-byte chunks - CONVERTTOUPPER process: Converts text to uppercase using tr command - Workflow: Connects processes with .flatten() to handle multiple outputs - Output: Uses .view{ it } to display results\n\n\n\nOne of the features of Nextflow is the ability to override parameters from the command line. We can change the greeting message without modifying the script:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\n\n\n\nWe also worked with building our own nextflow pipelines for quality control of rna-seq data. The training included running FastQC and MultiQC on sample data:\n\n\n\nChannel Process FastQC\n\n\nFigure 2: Nextflow channel and process architecture for FastQC quality control. This diagram shows how input data flows through channels into FastQC processes. Each process can run independently while channels manage data flow between steps.\n\n\n\nFastQC Report - Sample 1 - Individual quality control report for gut sample 1\nFastQC Report - Sample 2 - Individual quality control report for gut sample 2\n\nMultiQC Report - Comprehensive report combining all QC metrics\n\nThese reports demonstrate: - FastQC analysis: Per-base quality scores, sequence composition, adapter contamination - MultiQC aggregation: Combined visualization of multiple samples\n\nEnd of Day 3!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "August¬¥s mbi-blog",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 10, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 7, 2025\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4 of MedBioInfo\n\n\n\nmedbioinfo\n\nlearning\n\ndata-science\n\nnf-core\n\n\n\nDay 4 - Advanced nf-core pipelines and RNAseq analysis\n\n\n\n\n\nOct 28, 2024\n\n\nAugust Lundholm\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3 of MedBioInfo\n\n\n\nmedbioinfo\n\nlearning\n\ndata-science\n\n\n\nDay 3 reflections and learnings from medical bioinformatics course\n\n\n\n\n\nOct 28, 2024\n\n\nAugust Lundholm\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2 of applied bioinformatics\n\n\n\nmedbioinfo\n\nlearning\n\ndata-science\n\n\n\nContinuing my journey through applied bioinformatics - Day 2\n\n\n\n\n\nOct 27, 2024\n\n\nAugust Lundholm\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\nblog\n\n\n\nReflections and notes from day 1.\n\n\n\n\n\nJun 10, 2024\n\n\nAugust Lundholm\n\n\n\n\n\nNo matching items"
  }
]