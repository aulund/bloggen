[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/day1/index.html",
    "href": "posts/day1/index.html",
    "title": "Day 1",
    "section": "",
    "text": "This is my first blog post! Here are my reflections and notes from day 1.\n\n\n\nWe talked about good practice in storing and processing data\nHow to set up a Quarto blog\nThe importance of YAML frontmatter\nHow to write content in Markdown\n\n\n\n\n\nWe discussed the data lifecycle and what different stages the data goes through. from planning for what data to be collected to eventually being reused. The data lifecycle can be long and involve multiple people and thats why its important to work according to FAIR-principles.\nFAIR - Findable Accesible Interoperable and Reusable. Working according to this concept ensure that data collected for one question can be used for many more.\n\nThis relies on good data management practices where you work by research documentation, data organisation, information security, ethics and legislation.\n\nData is often not complete after first collection, first processing or first analysis. things are being added or moved between project altered as the work goes along. which makes it important to store data in a good system in a separate folder of raw-data which you dont touch.\n\nsome tips for good practice with documenting your data:\n\nDocument your methods and workflows.\nDocument where and when you downloaded data.\nDocument the versions of the software that you ran.\n\n\nOther things that we will practice later in the course is:\n\nEmploying version control via GIT\nShareable environments to make your scripts reproducible.\ncontainers to run scripts on other operating systems\nworkflow managers, that keep track of all of the above and also parse your data through the different scripts.\n\n\n\n\n\n\nWe also learned to set up this blog!\nQuarto allows you to write text and code into the same document and render into a few different formats such as webpages, blogs (!), docx and pdf.\n\n\n\n\n\nhere is a python example:\n\n\n# A simple Python example\ngreeting = \"Hello, world!\"\nprint(greeting)\n\nHello, world!\n\n# Some basic math\nnumbers = [1, 2, 3, 4, 5]\ntotal = sum(numbers)\nprint(f\"The sum is: {total}\")\n\nThe sum is: 15\n\n\n\nand this is a R example!\n\n\ndata(mtcars)\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThat’s all for today!"
  },
  {
    "objectID": "posts/day1/index.html#what-i-learned-today",
    "href": "posts/day1/index.html#what-i-learned-today",
    "title": "Day 1",
    "section": "",
    "text": "We talked about good practice in storing and processing data\nHow to set up a Quarto blog\nThe importance of YAML frontmatter\nHow to write content in Markdown"
  },
  {
    "objectID": "posts/day1/index.html#data-management-and-reproducible-research",
    "href": "posts/day1/index.html#data-management-and-reproducible-research",
    "title": "Day 1",
    "section": "",
    "text": "We discussed the data lifecycle and what different stages the data goes through. from planning for what data to be collected to eventually being reused. The data lifecycle can be long and involve multiple people and thats why its important to work according to FAIR-principles.\nFAIR - Findable Accesible Interoperable and Reusable. Working according to this concept ensure that data collected for one question can be used for many more.\n\nThis relies on good data management practices where you work by research documentation, data organisation, information security, ethics and legislation.\n\nData is often not complete after first collection, first processing or first analysis. things are being added or moved between project altered as the work goes along. which makes it important to store data in a good system in a separate folder of raw-data which you dont touch.\n\nsome tips for good practice with documenting your data:\n\nDocument your methods and workflows.\nDocument where and when you downloaded data.\nDocument the versions of the software that you ran.\n\n\nOther things that we will practice later in the course is:\n\nEmploying version control via GIT\nShareable environments to make your scripts reproducible.\ncontainers to run scripts on other operating systems\nworkflow managers, that keep track of all of the above and also parse your data through the different scripts."
  },
  {
    "objectID": "posts/day1/index.html#quarto-blog",
    "href": "posts/day1/index.html#quarto-blog",
    "title": "Day 1",
    "section": "",
    "text": "We also learned to set up this blog!\nQuarto allows you to write text and code into the same document and render into a few different formats such as webpages, blogs (!), docx and pdf."
  },
  {
    "objectID": "posts/day1/index.html#here-are-some-samples-of-code",
    "href": "posts/day1/index.html#here-are-some-samples-of-code",
    "title": "Day 1",
    "section": "",
    "text": "here is a python example:\n\n\n# A simple Python example\ngreeting = \"Hello, world!\"\nprint(greeting)\n\nHello, world!\n\n# Some basic math\nnumbers = [1, 2, 3, 4, 5]\ntotal = sum(numbers)\nprint(f\"The sum is: {total}\")\n\nThe sum is: 15\n\n\n\nand this is a R example!\n\n\ndata(mtcars)\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThat’s all for today!"
  },
  {
    "objectID": "posts/day2/index.html",
    "href": "posts/day2/index.html",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Today we covered four main topics: - Introduction to environments - Quality control with Pixi and Slurm - Introduction to containers\n- Quality control with containers\n\n\n\n-Since I work with variant visualisations, large datasets, and a mix of custom scripts and publication-ready figures it’s important that my toolchain is reproducible.\n-Pixi keeps everything isolated and version-controlled, so I don’t end up with “it works on my computer” issues. -A dedicated environment for each project makes it easier to share with others to get the same exact tools and versions as when i made the analysis. -When I prepare figures or analyses for papers, Pixi can help me keep track of software versions, channels, and dependencies. -Pixi supports multiple platforms and I can test everything locally and then run it on the cluster.\n\n\n\n\nQuality control is one of the most important parts of bioinformatics.\nIf the sequencing data is bad, everything that comes after will be unreliable.\nRunning FastQC and MultiQC early on saves time later.\nHaving a clear directory structure and running everything inside a controlled environment like Pixi keeps track of what was actually done.\nWorking with SLURM and screen was a bit difficult at first, but it’s useful to understand how jobs run on the cluster\n\n\n\n\n-Lack of reproducibility is a common problem in bioinformatics. -Containers make it more manageable. They keep analyses stable even when systems or dependencies change. -It’s also a practical way to share workflows without needing to explain every installation step.\n\n*That’s all for today"
  },
  {
    "objectID": "posts/day2/index.html#todays-focus",
    "href": "posts/day2/index.html#todays-focus",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Today we covered four main topics: - Introduction to environments - Quality control with Pixi and Slurm - Introduction to containers\n- Quality control with containers"
  },
  {
    "objectID": "posts/day2/index.html#introduction-to-environments",
    "href": "posts/day2/index.html#introduction-to-environments",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "-Since I work with variant visualisations, large datasets, and a mix of custom scripts and publication-ready figures it’s important that my toolchain is reproducible.\n-Pixi keeps everything isolated and version-controlled, so I don’t end up with “it works on my computer” issues. -A dedicated environment for each project makes it easier to share with others to get the same exact tools and versions as when i made the analysis. -When I prepare figures or analyses for papers, Pixi can help me keep track of software versions, channels, and dependencies. -Pixi supports multiple platforms and I can test everything locally and then run it on the cluster."
  },
  {
    "objectID": "posts/day2/index.html#quality-control-with-pixi-and-slurm",
    "href": "posts/day2/index.html#quality-control-with-pixi-and-slurm",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Quality control is one of the most important parts of bioinformatics.\nIf the sequencing data is bad, everything that comes after will be unreliable.\nRunning FastQC and MultiQC early on saves time later.\nHaving a clear directory structure and running everything inside a controlled environment like Pixi keeps track of what was actually done.\nWorking with SLURM and screen was a bit difficult at first, but it’s useful to understand how jobs run on the cluster"
  },
  {
    "objectID": "posts/day2/index.html#introduction-to-containers",
    "href": "posts/day2/index.html#introduction-to-containers",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "-Lack of reproducibility is a common problem in bioinformatics. -Containers make it more manageable. They keep analyses stable even when systems or dependencies change. -It’s also a practical way to share workflows without needing to explain every installation step.\n\n*That’s all for today"
  },
  {
    "objectID": "posts/day4/index.html",
    "href": "posts/day4/index.html",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "Today we moved from basic Nextflow to more advanced pipeline usage with real biological data.\n\n\nToday we worked with: - nf-core pipeline introduction and evaluation - Testing pipelines with built-in test data\n- Running the nf-core/rnaseq pipeline on real data\n\n\n\n\n\nThe nf-core community provides: - Standardized pipelines for many bioinformatics workflows - Extensive documentation following consistent guidelines - Automatic input validation through the nf-core launcher - Consistent structure across all pipelines - Open source development by volunteer community\n\n\n\nWe explored the nf-core homepage to find pipelines to use for our own projects\n\n\n\n\n\n\nFirst, we created a dedicated pixi environment for nf-core work:\npixi init nextflow_test -c conda-forge -c bioconda\ncd nextflow_test\npixi add nextflow nf-core\n\n\n\nWe verified our installation worked correctly:\npixi run nextflow -version\npixi run nf-core --help\npixi run nextflow run hello\nExpected output:\nN E X T F L O W\nversion 25.04.7 build 5955\ncreated 08-09-2025 13:29 UTC (15:29 CEST)\ncite doi:10.1038/nbt.3820\nhttp://nextflow.io\n\n\n\nWe downloaded the HPC2N configuration file for server-specific settings:\nwget https://raw.githubusercontent.com/hpc2n/intro-course/master/exercises/NEXTFLOW/INTERACTIVE/hpc2n.config\nKey configuration parameters: - max_memory = 128.GB - max_cpus = 28 - max_time = 168.h - executor = 'slurm'\n\n\n\nWe tested the Sarek variant calling pipeline with built-in test data:\npixi run nextflow run nf-core/sarek -profile test --outdir sarek_test -c hpc2n.config\n\n\n\nwe worked with RNAseq data located at:\nmedbioinfo2025/common_data/RNAseq\nWe created symbolic links to organize our data:\nmkdir data\ncd data\nln -s ../../common_data/RNAseq/*.fastq.gz .\ncd ..\nProject structure:\n.\n├── data\n│   ├── SRR5223504_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_1.fastq.gz\n│   ├── SRR5223504_2.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_2.fastq.gz\n│   ├── SRR5223517_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223517_1.fastq.gz\n│   └── ... (additional samples)\n├── pixi.lock\n└── pixi.toml\n\n\n\nTwo execution methods:\n\n\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\n\n\n\n#!/bin/bash -l\n#SBATCH -A our_proj_allocation\n#SBATCH -n 5\n#SBATCH -t 24:00:00\n\n/your_home_directory/.pixi/bin/pixi run nextflow run nf-core/rnaseq -r 3.19.0 -params-file /your_path/nf-params.json -c server.config\n\n\n\n\n\n\nnf-core homepage\nnf-core/rnaseq pipeline\nnf-core/sarek pipeline\n\n\nDay 4 complete!"
  },
  {
    "objectID": "posts/day4/index.html#todays-focus",
    "href": "posts/day4/index.html#todays-focus",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "Today we worked with: - nf-core pipeline introduction and evaluation - Testing pipelines with built-in test data\n- Running the nf-core/rnaseq pipeline on real data"
  },
  {
    "objectID": "posts/day4/index.html#morning-session---nf-core-introduction",
    "href": "posts/day4/index.html#morning-session---nf-core-introduction",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "The nf-core community provides: - Standardized pipelines for many bioinformatics workflows - Extensive documentation following consistent guidelines - Automatic input validation through the nf-core launcher - Consistent structure across all pipelines - Open source development by volunteer community\n\n\n\nWe explored the nf-core homepage to find pipelines to use for our own projects"
  },
  {
    "objectID": "posts/day4/index.html#afternoon-session---testing-a-pipeline",
    "href": "posts/day4/index.html#afternoon-session---testing-a-pipeline",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "First, we created a dedicated pixi environment for nf-core work:\npixi init nextflow_test -c conda-forge -c bioconda\ncd nextflow_test\npixi add nextflow nf-core\n\n\n\nWe verified our installation worked correctly:\npixi run nextflow -version\npixi run nf-core --help\npixi run nextflow run hello\nExpected output:\nN E X T F L O W\nversion 25.04.7 build 5955\ncreated 08-09-2025 13:29 UTC (15:29 CEST)\ncite doi:10.1038/nbt.3820\nhttp://nextflow.io\n\n\n\nWe downloaded the HPC2N configuration file for server-specific settings:\nwget https://raw.githubusercontent.com/hpc2n/intro-course/master/exercises/NEXTFLOW/INTERACTIVE/hpc2n.config\nKey configuration parameters: - max_memory = 128.GB - max_cpus = 28 - max_time = 168.h - executor = 'slurm'\n\n\n\nWe tested the Sarek variant calling pipeline with built-in test data:\npixi run nextflow run nf-core/sarek -profile test --outdir sarek_test -c hpc2n.config\n\n\n\nwe worked with RNAseq data located at:\nmedbioinfo2025/common_data/RNAseq\nWe created symbolic links to organize our data:\nmkdir data\ncd data\nln -s ../../common_data/RNAseq/*.fastq.gz .\ncd ..\nProject structure:\n.\n├── data\n│   ├── SRR5223504_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_1.fastq.gz\n│   ├── SRR5223504_2.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_2.fastq.gz\n│   ├── SRR5223517_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223517_1.fastq.gz\n│   └── ... (additional samples)\n├── pixi.lock\n└── pixi.toml\n\n\n\nTwo execution methods:\n\n\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\n\n\n\n#!/bin/bash -l\n#SBATCH -A our_proj_allocation\n#SBATCH -n 5\n#SBATCH -t 24:00:00\n\n/your_home_directory/.pixi/bin/pixi run nextflow run nf-core/rnaseq -r 3.19.0 -params-file /your_path/nf-params.json -c server.config"
  },
  {
    "objectID": "posts/day4/index.html#resources",
    "href": "posts/day4/index.html#resources",
    "title": "Day 4 of MedBioInfo",
    "section": "",
    "text": "nf-core homepage\nnf-core/rnaseq pipeline\nnf-core/sarek pipeline\n\n\nDay 4 complete!"
  },
  {
    "objectID": "posts/day3/index.html",
    "href": "posts/day3/index.html",
    "title": "Day 3 of MedBioInfo",
    "section": "",
    "text": "Today we worked with Nextflow and workflow management.\n\n\n\nNextflow Overview\n\n\nFigure 1: Nextflow workflow management system overview. This diagram illustrates the core concepts of Nextflow including processes, channels, and how data flows through a computational pipeline.\n\n\nToday we ran our first Nextflow script:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nWhen we run this script:\nnextflow run hello.nf\nWe get this output:\nN E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ✔\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ✔\nHELLO \nWORLD!\n\n\n\nOne of the features of Nextflow is the ability to override parameters from the command line. We can change the greeting message without modifying the script:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\n\n\n\nWe also worked with building our own nextflow pipelines for quality control of rna-seq data. The training included running FastQC and MultiQC on sample data:\n\n\n\nChannel Process FastQC\n\n\nFigure 2: Nextflow channel and process architecture for FastQC quality control. This diagram shows how input data flows through channels into FastQC processes. Each process can run independently while channels manage data flow between steps.\n\n\n\nFastQC Report - Sample 1 - Individual quality control report for gut sample 1\nFastQC Report - Sample 2 - Individual quality control report for gut sample 2\n\nMultiQC Report - Comprehensive report combining all QC metrics\n\n\nEnd of Day 3!"
  },
  {
    "objectID": "posts/day3/index.html#todays-focus",
    "href": "posts/day3/index.html#todays-focus",
    "title": "Day 3 of MedBioInfo",
    "section": "",
    "text": "Today we worked with Nextflow and workflow management.\n\n\n\nNextflow Overview\n\n\nFigure 1: Nextflow workflow management system overview. This diagram illustrates the core concepts of Nextflow including processes, channels, and how data flows through a computational pipeline.\n\n\nToday we ran our first Nextflow script:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nWhen we run this script:\nnextflow run hello.nf\nWe get this output:\nN E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ✔\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ✔\nHELLO \nWORLD!\n\n\n\nOne of the features of Nextflow is the ability to override parameters from the command line. We can change the greeting message without modifying the script:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\n\n\n\nWe also worked with building our own nextflow pipelines for quality control of rna-seq data. The training included running FastQC and MultiQC on sample data:\n\n\n\nChannel Process FastQC\n\n\nFigure 2: Nextflow channel and process architecture for FastQC quality control. This diagram shows how input data flows through channels into FastQC processes. Each process can run independently while channels manage data flow between steps.\n\n\n\nFastQC Report - Sample 1 - Individual quality control report for gut sample 1\nFastQC Report - Sample 2 - Individual quality control report for gut sample 2\n\nMultiQC Report - Comprehensive report combining all QC metrics\n\n\nEnd of Day 3!"
  },
  {
    "objectID": "posts/day5/index.html",
    "href": "posts/day5/index.html",
    "title": "Day 5 of MedBioInfo",
    "section": "",
    "text": "Day 5 we had an important discussion about AI in bioinformatics (actually the discussion were onday 4 because of us maxing out our allocated processing power on hp2nc and things were moved around) and a lecture + excerzises in data visualization with ggplot\n\n\nToday we covered two topics: - AI in bioinformatics - about the benefits, risks, and responsible usage - Data visualization with ggplot\n\n\n\n\n ### The AI Revolution\nIn the last 3 years, large language models have exploded in popularity and the first one, ChatGPT gained 100 million users in 2 months after its release. That makes llms the fastest-growing consumer application in history.\n\n\nMachine learning has been used in bioinformatics for decades: - Classification and regression on annotated data - Pattern discovery without human input - Clustering, association, dimensionality reduction\nGenAI represents the next evolutionary step in applying machine learning into our lives.\n\n\n\n\n\n\nAI models inherit biases from their training data: - Historical bias - Internet bias\n\nAI is not trained to be wrong and will “happily hallucinate an answer” rather than admit uncertainty\n\n\n\n\nLearning concerns: - 2025 study showed adults using ChatGPT for essays were consistently outperformed by those writing without AI - Significantly lower brain engagement when using AI assistance\nProductivity paradox: - Study of experienced developers (5+ years) found AI tools actually reduced productivity - Time spent on prompting, reviewing AI responses, and being idle - Less time actually coding and problem-solving\n\n\n\nMETR study results showing AI impact on developer productivity\n\n\n\n\n\nCritical concerns for bioinformatics: - Biological data is highly sensitive and often legally protected - Terms of Service violations - Copyright issues - GDPR compliance\nBest practices: - Check Terms of Service explicitly and frequently - Use GDPR-compliant tools when possible - Share data descriptions, not actual datasets - Be wary of browser extensions that steal personal data\n\n\n\nResource consumption: - Data centers use more electricity than many countries - Massive water requirements for cooling - Built near poor communities, often draining local resources\n\n\n\nAI is already changing language patterns: - Words like “delve” and “meticulous” increasingly common in academic talks. - Different AI models have distinct writing styles. - 24% of internet content was AI-generated by end of 2023 from 2-3% in 2022.\n\n\n\nChatGPT’s impact on academic language use\n\n\n\n\n\n\n\n\n\nAm I phrasing my prompt effectively?\nCan I find this information another way?\nHow much time am I actually saving?\nDo I know enough to identify AI errors?\nWhat are the consequences of testing AI solutions?\n\n\n\n\n\n\n\nAfter completing analyses, visualization becomes crucial\nggplot2 is R’s visualization package\n\n\n\nWe worked through practical examples using the NBIS tutorial by Roy Francis, covering:\n# Basic ggplot structure\nlibrary(ggplot2)\n\n# Example scatter plot using iris dataset\nggplot(data = iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"My Analysis Results\",\n       x = \"Petal Length\",\n       y = \"Petal Width\")\n\n\n\nBasic scatter plot example\n\n\n# Advanced plotting with facets using iris dataset\nggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  facet_wrap(~Species) +\n  theme_bw()\n\n\n\nFaceted scatter plot with color grouping\n\n\n\n\n\nWe also worked through two complex examples from the NBIS tutorial that demonstrate professional-level data visualization techniques:\n\n\nThis recreation of The Economist’s corruption analysis demonstrates advanced ggplot2 techniques:\n# Read and process the data\nec &lt;- read.csv(\"data_economist.csv\", header = TRUE)\n\n# Refactor regions with line breaks for legend\nec$Region &lt;- factor(ec$Region,\n                    levels = c(\"EU W. Europe\", \"Americas\", \"Asia Pacific\",\n                              \"East EU Cemt Asia\", \"MENA\", \"SSA\"),\n                    labels = c(\"OECD\", \"Americas\", \"Asia &\\nOceania\",\n                              \"Central &\\nEastern Europe\",\n                              \"Middle East &\\nNorth Africa\",\n                              \"Sub-Saharan\\nAfrica\"))\n\n# Create sophisticated scatter plot with trend line\nggplot(ec, aes(x = CPI, y = HDI, color = Region)) +\n  geom_smooth(aes(fill = \"red\"), method = \"lm\", formula = y ~ poly(x, 2), \n              se = FALSE, color = \"red\", linewidth = 0.6) +\n  geom_point(shape = 21, size = 3, stroke = 0.8, fill = \"white\") +\n  geom_text(data = subset(ec, Country %in% labels), aes(label = Country),\n            color = \"black\", size = 2.5) +\n  scale_color_manual(values = c(\"#23576E\", \"#099FDB\", \"#29B00E\", \n                               \"#208F84\", \"#F55840\", \"#924F3E\")) +\n  labs(title = \"Corruption and human development\") +\n  theme_minimal()\n\n\n\nCorruption and Human Development Analysis\n\n\n\n\n\nThis recreation of WSJ’s measles vaccination impact visualization shows complex data transformation and heatmap creation:\n# Read and transform the complex dataset\nme &lt;- read.csv(\"data_wsj.csv\", header = TRUE, stringsAsFactors = FALSE, skip = 2)\n\n# Custom function for handling missing data\nfun1 &lt;- function(x) ifelse(all(is.na(x)), NA, sum(x, na.rm = TRUE))\n\n# Complex data transformation pipeline\nme3 &lt;- me %&gt;%\n  gather(key = state, value = value, -YEAR, -WEEK) %&gt;%\n  mutate(value = str_replace(value, \"^-$\", NA_character_),\n         value = as.numeric(value)) %&gt;%\n  group_by(YEAR, state) %&gt;% \n  summarise(total = fun1(value), .groups = \"drop\") %&gt;%\n  mutate(state = str_replace_all(state, \"[.]\", \" \"),\n         state = str_to_title(state))\n\n# Create sophisticated heatmap with custom color scale\nggplot(me3, aes(x = year, y = reorder(state, desc(state)), fill = total)) +\n  geom_tile(color = \"white\", linewidth = 0.25) +\n  scale_fill_gradientn(colors = cols, na.value = \"grey95\",\n                       limits = c(0, 4000),\n                       values = c(0, 0.01, 0.02, 0.03, 0.09, 0.1, 0.15, 0.25, 0.4, 0.5, 1)) +\n  geom_segment(x = 1963, xend = 1963, y = 0, yend = 51.5, linewidth = 0.6) +\n  annotate(\"text\", label = \"Vaccine introduced\", x = 1963, y = 53) +\n  coord_fixed() +\n  theme_minimal()\n\n\n\nMeasles Cases Heatmap\n\n\n\n\n\n\n\n\nNBIS ggplot Tutorial\n\n\nDay 5 complete and the course done! Thank you amrei for a really good and interesting course!"
  },
  {
    "objectID": "posts/day5/index.html#todays-focus",
    "href": "posts/day5/index.html#todays-focus",
    "title": "Day 5 of MedBioInfo",
    "section": "",
    "text": "Today we covered two topics: - AI in bioinformatics - about the benefits, risks, and responsible usage - Data visualization with ggplot"
  },
  {
    "objectID": "posts/day5/index.html#morning-session---ai-in-bioinformatics",
    "href": "posts/day5/index.html#morning-session---ai-in-bioinformatics",
    "title": "Day 5 of MedBioInfo",
    "section": "",
    "text": "### The AI Revolution\nIn the last 3 years, large language models have exploded in popularity and the first one, ChatGPT gained 100 million users in 2 months after its release. That makes llms the fastest-growing consumer application in history.\n\n\nMachine learning has been used in bioinformatics for decades: - Classification and regression on annotated data - Pattern discovery without human input - Clustering, association, dimensionality reduction\nGenAI represents the next evolutionary step in applying machine learning into our lives."
  },
  {
    "objectID": "posts/day5/index.html#critical-issues-with-ai",
    "href": "posts/day5/index.html#critical-issues-with-ai",
    "title": "Day 5 of MedBioInfo",
    "section": "",
    "text": "AI models inherit biases from their training data: - Historical bias - Internet bias\n\nAI is not trained to be wrong and will “happily hallucinate an answer” rather than admit uncertainty\n\n\n\n\nLearning concerns: - 2025 study showed adults using ChatGPT for essays were consistently outperformed by those writing without AI - Significantly lower brain engagement when using AI assistance\nProductivity paradox: - Study of experienced developers (5+ years) found AI tools actually reduced productivity - Time spent on prompting, reviewing AI responses, and being idle - Less time actually coding and problem-solving\n\n\n\nMETR study results showing AI impact on developer productivity\n\n\n\n\n\nCritical concerns for bioinformatics: - Biological data is highly sensitive and often legally protected - Terms of Service violations - Copyright issues - GDPR compliance\nBest practices: - Check Terms of Service explicitly and frequently - Use GDPR-compliant tools when possible - Share data descriptions, not actual datasets - Be wary of browser extensions that steal personal data\n\n\n\nResource consumption: - Data centers use more electricity than many countries - Massive water requirements for cooling - Built near poor communities, often draining local resources\n\n\n\nAI is already changing language patterns: - Words like “delve” and “meticulous” increasingly common in academic talks. - Different AI models have distinct writing styles. - 24% of internet content was AI-generated by end of 2023 from 2-3% in 2022.\n\n\n\nChatGPT’s impact on academic language use"
  },
  {
    "objectID": "posts/day5/index.html#responsible-ai-usage-guidelines",
    "href": "posts/day5/index.html#responsible-ai-usage-guidelines",
    "title": "Day 5 of MedBioInfo",
    "section": "",
    "text": "Am I phrasing my prompt effectively?\nCan I find this information another way?\nHow much time am I actually saving?\nDo I know enough to identify AI errors?\nWhat are the consequences of testing AI solutions?"
  },
  {
    "objectID": "posts/day5/index.html#afternoon-session---data-visualization-with-ggplot",
    "href": "posts/day5/index.html#afternoon-session---data-visualization-with-ggplot",
    "title": "Day 5 of MedBioInfo",
    "section": "",
    "text": "After completing analyses, visualization becomes crucial\nggplot2 is R’s visualization package\n\n\n\nWe worked through practical examples using the NBIS tutorial by Roy Francis, covering:\n# Basic ggplot structure\nlibrary(ggplot2)\n\n# Example scatter plot using iris dataset\nggplot(data = iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"My Analysis Results\",\n       x = \"Petal Length\",\n       y = \"Petal Width\")\n\n\n\nBasic scatter plot example\n\n\n# Advanced plotting with facets using iris dataset\nggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  facet_wrap(~Species) +\n  theme_bw()\n\n\n\nFaceted scatter plot with color grouping\n\n\n\n\n\nWe also worked through two complex examples from the NBIS tutorial that demonstrate professional-level data visualization techniques:\n\n\nThis recreation of The Economist’s corruption analysis demonstrates advanced ggplot2 techniques:\n# Read and process the data\nec &lt;- read.csv(\"data_economist.csv\", header = TRUE)\n\n# Refactor regions with line breaks for legend\nec$Region &lt;- factor(ec$Region,\n                    levels = c(\"EU W. Europe\", \"Americas\", \"Asia Pacific\",\n                              \"East EU Cemt Asia\", \"MENA\", \"SSA\"),\n                    labels = c(\"OECD\", \"Americas\", \"Asia &\\nOceania\",\n                              \"Central &\\nEastern Europe\",\n                              \"Middle East &\\nNorth Africa\",\n                              \"Sub-Saharan\\nAfrica\"))\n\n# Create sophisticated scatter plot with trend line\nggplot(ec, aes(x = CPI, y = HDI, color = Region)) +\n  geom_smooth(aes(fill = \"red\"), method = \"lm\", formula = y ~ poly(x, 2), \n              se = FALSE, color = \"red\", linewidth = 0.6) +\n  geom_point(shape = 21, size = 3, stroke = 0.8, fill = \"white\") +\n  geom_text(data = subset(ec, Country %in% labels), aes(label = Country),\n            color = \"black\", size = 2.5) +\n  scale_color_manual(values = c(\"#23576E\", \"#099FDB\", \"#29B00E\", \n                               \"#208F84\", \"#F55840\", \"#924F3E\")) +\n  labs(title = \"Corruption and human development\") +\n  theme_minimal()\n\n\n\nCorruption and Human Development Analysis\n\n\n\n\n\nThis recreation of WSJ’s measles vaccination impact visualization shows complex data transformation and heatmap creation:\n# Read and transform the complex dataset\nme &lt;- read.csv(\"data_wsj.csv\", header = TRUE, stringsAsFactors = FALSE, skip = 2)\n\n# Custom function for handling missing data\nfun1 &lt;- function(x) ifelse(all(is.na(x)), NA, sum(x, na.rm = TRUE))\n\n# Complex data transformation pipeline\nme3 &lt;- me %&gt;%\n  gather(key = state, value = value, -YEAR, -WEEK) %&gt;%\n  mutate(value = str_replace(value, \"^-$\", NA_character_),\n         value = as.numeric(value)) %&gt;%\n  group_by(YEAR, state) %&gt;% \n  summarise(total = fun1(value), .groups = \"drop\") %&gt;%\n  mutate(state = str_replace_all(state, \"[.]\", \" \"),\n         state = str_to_title(state))\n\n# Create sophisticated heatmap with custom color scale\nggplot(me3, aes(x = year, y = reorder(state, desc(state)), fill = total)) +\n  geom_tile(color = \"white\", linewidth = 0.25) +\n  scale_fill_gradientn(colors = cols, na.value = \"grey95\",\n                       limits = c(0, 4000),\n                       values = c(0, 0.01, 0.02, 0.03, 0.09, 0.1, 0.15, 0.25, 0.4, 0.5, 1)) +\n  geom_segment(x = 1963, xend = 1963, y = 0, yend = 51.5, linewidth = 0.6) +\n  annotate(\"text\", label = \"Vaccine introduced\", x = 1963, y = 53) +\n  coord_fixed() +\n  theme_minimal()\n\n\n\nMeasles Cases Heatmap"
  },
  {
    "objectID": "posts/day5/index.html#resources-and-references",
    "href": "posts/day5/index.html#resources-and-references",
    "title": "Day 5 of MedBioInfo",
    "section": "",
    "text": "NBIS ggplot Tutorial\n\n\nDay 5 complete and the course done! Thank you amrei for a really good and interesting course!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "August´s mbi-blog",
    "section": "",
    "text": "Day 5 of MedBioInfo\n\n\n\nmedbioinfo\n\nlearning\n\ndata-science\n\nAI\n\nvisualization\n\n\n\nDay 5 - AI in Bioinformatics and Data Visualization with ggplot\n\n\n\n\n\nOct 10, 2025\n\n\nAugust Lundholm\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4 of MedBioInfo\n\n\n\nmedbioinfo\n\nlearning\n\ndata-science\n\nnf-core\n\n\n\nDay 4 - nf-core pipelines and RNAseq analysis\n\n\n\n\n\nOct 9, 2025\n\n\nAugust Lundholm\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3 of MedBioInfo\n\n\n\nmedbioinfo\n\nlearning\n\ndata-science\n\n\n\nDay 3 of applied bioinformatics course\n\n\n\n\n\nOct 8, 2025\n\n\nAugust Lundholm\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2 of applied bioinformatics\n\n\n\nmedbioinfo\n\nlearning\n\ndata-science\n\n\n\nContinuing applied bioinformatics\n\n\n\n\n\nOct 7, 2025\n\n\nAugust Lundholm\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\nblog\n\n\n\nnotes from day 1.\n\n\n\n\n\nOct 6, 2025\n\n\nAugust Lundholm\n\n\n\n\n\nNo matching items"
  }
]