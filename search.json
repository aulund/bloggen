[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/day1/index.html",
    "href": "posts/day1/index.html",
    "title": "Day 1",
    "section": "",
    "text": "This is my first blog post! Here are my reflections and notes from day 1.\n\n\n\nWe talked about good practice in storing and processing data\nHow to set up a Quarto blog\nThe importance of YAML frontmatter\nHow to write content in Markdown\n\n\n\n\n\nWe discussed the data lifecycle and what different stages the data goes through. from planning for what data to be collected to eventually being reused. The data lifecycle can be long and involve multiple people and thats why its important to work according to FAIR-principles.\nFAIR - Findable Accesible Interoperable and Reusable. Working according to this concept ensure that data collected for one question can be used for many more.\n\nThis relies on good data management practices where you work by research documentation, data organisation, information security, ethics and legislation.\n\nData is often not complete after first collection, first processing or first analysis. things are being added or moved between project altered as the work goes along. which makes it important to store data in a good system in a separate folder of raw-data which you dont touch.\n\nsome tips for good practice with documenting your data:\n\nDocument your methods and workflows.\nDocument where and when you downloaded data.\nDocument the versions of the software that you ran.\n\n\nOther things that we will practice later in the course is:\n\nEmploying version control via GIT\nShareable environments to make your scripts reproducible.\ncontainers to run scripts on other operating systems\nworkflow managers, that keep track of all of the above and also parse your data through the different scripts.\n\n\n\n\n\n\nWe also learned to set up this blog!\nQuarto allows you to write text and code into the same document and render into a few different formats such as webpages, blogs (!), docx and pdf.\n\n\n\n\n\nhere is a python example:\n\n\n# A simple Python example\ngreeting = \"Hello, world!\"\nprint(greeting)\n\nHello, world!\n\n# Some basic math\nnumbers = [1, 2, 3, 4, 5]\ntotal = sum(numbers)\nprint(f\"The sum is: {total}\")\n\nThe sum is: 15\n\n\n\nand this is a R example!\n\n\ndata(mtcars)\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThat’s all for today!"
  },
  {
    "objectID": "posts/day1/index.html#what-i-learned-today",
    "href": "posts/day1/index.html#what-i-learned-today",
    "title": "Day 1",
    "section": "",
    "text": "We talked about good practice in storing and processing data\nHow to set up a Quarto blog\nThe importance of YAML frontmatter\nHow to write content in Markdown"
  },
  {
    "objectID": "posts/day1/index.html#data-management-and-reproducible-research",
    "href": "posts/day1/index.html#data-management-and-reproducible-research",
    "title": "Day 1",
    "section": "",
    "text": "We discussed the data lifecycle and what different stages the data goes through. from planning for what data to be collected to eventually being reused. The data lifecycle can be long and involve multiple people and thats why its important to work according to FAIR-principles.\nFAIR - Findable Accesible Interoperable and Reusable. Working according to this concept ensure that data collected for one question can be used for many more.\n\nThis relies on good data management practices where you work by research documentation, data organisation, information security, ethics and legislation.\n\nData is often not complete after first collection, first processing or first analysis. things are being added or moved between project altered as the work goes along. which makes it important to store data in a good system in a separate folder of raw-data which you dont touch.\n\nsome tips for good practice with documenting your data:\n\nDocument your methods and workflows.\nDocument where and when you downloaded data.\nDocument the versions of the software that you ran.\n\n\nOther things that we will practice later in the course is:\n\nEmploying version control via GIT\nShareable environments to make your scripts reproducible.\ncontainers to run scripts on other operating systems\nworkflow managers, that keep track of all of the above and also parse your data through the different scripts."
  },
  {
    "objectID": "posts/day1/index.html#quarto-blog",
    "href": "posts/day1/index.html#quarto-blog",
    "title": "Day 1",
    "section": "",
    "text": "We also learned to set up this blog!\nQuarto allows you to write text and code into the same document and render into a few different formats such as webpages, blogs (!), docx and pdf."
  },
  {
    "objectID": "posts/day1/index.html#here-are-some-samples-of-code",
    "href": "posts/day1/index.html#here-are-some-samples-of-code",
    "title": "Day 1",
    "section": "",
    "text": "here is a python example:\n\n\n# A simple Python example\ngreeting = \"Hello, world!\"\nprint(greeting)\n\nHello, world!\n\n# Some basic math\nnumbers = [1, 2, 3, 4, 5]\ntotal = sum(numbers)\nprint(f\"The sum is: {total}\")\n\nThe sum is: 15\n\n\n\nand this is a R example!\n\n\ndata(mtcars)\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThat’s all for today!"
  },
  {
    "objectID": "posts/day2/index.html",
    "href": "posts/day2/index.html",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Welcome back to my learning journey! Here are my reflections and key takeaways from day 2.\n\n\nToday we covered four main topics: - Introduction to environments - Quality control with Pixi and Slurm - Introduction to containers\n- Quality control with containers\n\n\n\nSince I work with variant visualisations, large datasets, and a mix of custom scripts and publication-ready figures it’s crucial that my toolchain is consistent and reproducible across systems and collaborators. Using Pixi (or another environment manager) keeps everything isolated and version-controlled, so I don’t end up with “it works on my computer” issues. Having a dedicated environment for each project makes it easier to share my setup — others can just clone the repository and get the exact same tools and versions. It also prevents older pipelines or scripts from breaking when dependencies update. When I prepare figures or analyses for papers, Pixi can help me keep track of software versions, channels, and dependencies — which makes the “Methods” section reproducible. Pixi supports multiple platforms and I can test everything locally and then run it on the cluster with minimal risk of environment mismatch.\n\n\n\nQuality control isn’t the most exciting part of bioinformatics, but it’s one of the most important. If the sequencing data is bad, everything that comes after will be unreliable. Running FastQC and MultiQC early on saves time later, even if the results themselves are pretty basic. It also makes me think more about how I organize things. Having a clear directory structure and running everything inside a controlled environment like Pixi helps keep track of what was actually done. It’s easy to lose that overview once the project grows. Working with SLURM and screen is a bit clunky at first, but it’s useful to understand how jobs run on the cluster. Being able to rerun things reproducibly — same data, same tools, same parameters — is really the main point.\n\n\n\nReproducibility is a constant problem in bioinformatics, and containers make it more manageable. They keep analyses stable even when systems or dependencies change. It’s also a practical way to share workflows without needing to explain every installation step. Building containers from scratch feels a bit advanced, but understanding how they work makes the overall workflow more reliable.\n\n*That’s all for today"
  },
  {
    "objectID": "posts/day2/index.html#todays-focus",
    "href": "posts/day2/index.html#todays-focus",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Today we covered four main topics: - Introduction to environments - Quality control with Pixi and Slurm - Introduction to containers\n- Quality control with containers"
  },
  {
    "objectID": "posts/day2/index.html#introduction-to-environments",
    "href": "posts/day2/index.html#introduction-to-environments",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Since I work with variant visualisations, large datasets, and a mix of custom scripts and publication-ready figures it’s crucial that my toolchain is consistent and reproducible across systems and collaborators. Using Pixi (or another environment manager) keeps everything isolated and version-controlled, so I don’t end up with “it works on my computer” issues. Having a dedicated environment for each project makes it easier to share my setup — others can just clone the repository and get the exact same tools and versions. It also prevents older pipelines or scripts from breaking when dependencies update. When I prepare figures or analyses for papers, Pixi can help me keep track of software versions, channels, and dependencies — which makes the “Methods” section reproducible. Pixi supports multiple platforms and I can test everything locally and then run it on the cluster with minimal risk of environment mismatch."
  },
  {
    "objectID": "posts/day2/index.html#quality-control-with-pixi-and-slurm",
    "href": "posts/day2/index.html#quality-control-with-pixi-and-slurm",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Quality control isn’t the most exciting part of bioinformatics, but it’s one of the most important. If the sequencing data is bad, everything that comes after will be unreliable. Running FastQC and MultiQC early on saves time later, even if the results themselves are pretty basic. It also makes me think more about how I organize things. Having a clear directory structure and running everything inside a controlled environment like Pixi helps keep track of what was actually done. It’s easy to lose that overview once the project grows. Working with SLURM and screen is a bit clunky at first, but it’s useful to understand how jobs run on the cluster. Being able to rerun things reproducibly — same data, same tools, same parameters — is really the main point."
  },
  {
    "objectID": "posts/day2/index.html#introduction-to-containers",
    "href": "posts/day2/index.html#introduction-to-containers",
    "title": "Day 2 of applied bioinformatics",
    "section": "",
    "text": "Reproducibility is a constant problem in bioinformatics, and containers make it more manageable. They keep analyses stable even when systems or dependencies change. It’s also a practical way to share workflows without needing to explain every installation step. Building containers from scratch feels a bit advanced, but understanding how they work makes the overall workflow more reliable.\n\n*That’s all for today"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/day3/index.html",
    "href": "posts/day3/index.html",
    "title": "Day 3 of MedBioInfo",
    "section": "",
    "text": "Welcome to Day 3! Here are my reflections and key takeaways from today’s sessions.\n\n\nToday we worked with Nextflow and workflow management.\n\n\n\nNextflow Overview\n\n\nFigure 1: Nextflow workflow management system overview. This diagram illustrates the core concepts of Nextflow including processes, channels, and how data flows through a computational pipeline. Nextflow enables scalable and portable bioinformatics workflows.\n\n\nToday we ran our first Nextflow script! Here’s the complete hello.nf script:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nWhen we run this script:\nnextflow run hello.nf\nWe get this output:\nN E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ✔\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ✔\nHELLO \nWORLD!\nThis workflow demonstrates several key Nextflow concepts: - Channels: greeting_ch carries data between processes - SPLITLETTERS process: Uses split command to break text into 6-byte chunks - CONVERTTOUPPER process: Converts text to uppercase using tr command - Workflow: Connects processes with .flatten() to handle multiple outputs - Output: Uses .view{ it } to display results\n\n\n\nOne of the powerful features of Nextflow is the ability to override parameters from the command line. We can change the greeting message without modifying the script:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\nThis demonstrates: - Parameter override: --greeting changes the default message - Resume functionality: -resume allows Nextflow to skip already completed tasks - Pixi integration: Using pixi run to manage the environment - Flexibility: Same workflow, different input data\n\n\n\nWe also worked with building our own nextflow pipelines for quality control of rna-seq data. The training included running FastQC and MultiQC on sample data:\n\n\n\nChannel Process FastQC\n\n\nFigure 2: Nextflow channel and process architecture for FastQC quality control. This diagram shows how input data flows through channels into FastQC processes. Each process can run independently while channels manage data flow between steps.\n\n\n\nFastQC Report - Sample 1 - Individual quality control report for gut sample 1\nFastQC Report - Sample 2 - Individual quality control report for gut sample 2\n\nMultiQC Report - Comprehensive report combining all QC metrics\n\nThese reports demonstrate: - FastQC analysis: Per-base quality scores, sequence composition, adapter contamination - MultiQC aggregation: Combined visualization of multiple samples - nf-core standards: Industry best practices for bioinformatics workflows\n\nEnd of Day 3!"
  },
  {
    "objectID": "posts/day3/index.html#todays-focus",
    "href": "posts/day3/index.html#todays-focus",
    "title": "Day 3 of MedBioInfo",
    "section": "",
    "text": "Today we worked with Nextflow and workflow management.\n\n\n\nNextflow Overview\n\n\nFigure 1: Nextflow workflow management system overview. This diagram illustrates the core concepts of Nextflow including processes, channels, and how data flows through a computational pipeline. Nextflow enables scalable and portable bioinformatics workflows.\n\n\nToday we ran our first Nextflow script! Here’s the complete hello.nf script:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nWhen we run this script:\nnextflow run hello.nf\nWe get this output:\nN E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ✔\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ✔\nHELLO \nWORLD!\nThis workflow demonstrates several key Nextflow concepts: - Channels: greeting_ch carries data between processes - SPLITLETTERS process: Uses split command to break text into 6-byte chunks - CONVERTTOUPPER process: Converts text to uppercase using tr command - Workflow: Connects processes with .flatten() to handle multiple outputs - Output: Uses .view{ it } to display results\n\n\n\nOne of the powerful features of Nextflow is the ability to override parameters from the command line. We can change the greeting message without modifying the script:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\nThis demonstrates: - Parameter override: --greeting changes the default message - Resume functionality: -resume allows Nextflow to skip already completed tasks - Pixi integration: Using pixi run to manage the environment - Flexibility: Same workflow, different input data\n\n\n\nWe also worked with building our own nextflow pipelines for quality control of rna-seq data. The training included running FastQC and MultiQC on sample data:\n\n\n\nChannel Process FastQC\n\n\nFigure 2: Nextflow channel and process architecture for FastQC quality control. This diagram shows how input data flows through channels into FastQC processes. Each process can run independently while channels manage data flow between steps.\n\n\n\nFastQC Report - Sample 1 - Individual quality control report for gut sample 1\nFastQC Report - Sample 2 - Individual quality control report for gut sample 2\n\nMultiQC Report - Comprehensive report combining all QC metrics\n\nThese reports demonstrate: - FastQC analysis: Per-base quality scores, sequence composition, adapter contamination - MultiQC aggregation: Combined visualization of multiple samples - nf-core standards: Industry best practices for bioinformatics workflows\n\nEnd of Day 3!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "August´s mbi-blog",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 10, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 7, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3 of MedBioInfo\n\n\n\nmedbioinfo\n\nlearning\n\ndata-science\n\n\n\nDay 3 reflections and learnings from medical bioinformatics course\n\n\n\n\n\nOct 28, 2024\n\n\nAugust Lundholm\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2 of applied bioinformatics\n\n\n\nmedbioinfo\n\nlearning\n\ndata-science\n\n\n\nContinuing my journey through applied bioinformatics - Day 2\n\n\n\n\n\nOct 27, 2024\n\n\nAugust Lundholm\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\nblog\n\n\n\nReflections and notes from day 1.\n\n\n\n\n\nJun 10, 2024\n\n\nAugust Lundholm\n\n\n\n\n\nNo matching items"
  }
]